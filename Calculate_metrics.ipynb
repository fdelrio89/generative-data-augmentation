{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "blind-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "responsible-brother",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_id(composit_image_id):\n",
    "    composit_image_id = composit_image_id.split('_')\n",
    "    if len(composit_image_id) > 2:\n",
    "        image_id = composit_image_id[-2]\n",
    "    else:\n",
    "        image_id = composit_image_id[-1]\n",
    "    return image_id\n",
    "\n",
    "def get_split(composit_image_id):\n",
    "    composit_image_id = composit_image_id.split('_')\n",
    "    if len(composit_image_id) > 2:\n",
    "        composit_split = composit_image_id[:-2]\n",
    "    else:\n",
    "        composit_split = composit_image_id[:-1]\n",
    "    \n",
    "    split = '_'.join(composit_split)\n",
    "    return split\n",
    "\n",
    "def group_results(raw_results):\n",
    "    results = {}\n",
    "    for element in raw_results:\n",
    "        image_id = get_image_id(element['image_id'])\n",
    "        split = get_split(element['image_id'])\n",
    "        results[image_id] = element['caption']\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "relevant-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def pre_caption(caption, max_words=0):\n",
    "    caption = re.sub(\n",
    "        r\"([.!\\\"()*#:;~])\",\n",
    "        ' ',\n",
    "        caption.lower(),\n",
    "    )\n",
    "    caption = re.sub(\n",
    "        r\"\\s{2,}\",\n",
    "        ' ',\n",
    "        caption,\n",
    "    )\n",
    "    caption = caption.rstrip('\\n')\n",
    "    caption = caption.strip(' ')\n",
    "\n",
    "    #truncate caption\n",
    "    caption_words = caption.split(' ')\n",
    "    if max_words and len(caption_words)>max_words:\n",
    "        caption = ' '.join(caption_words[:max_words])\n",
    "\n",
    "    return caption\n",
    "\n",
    "def load_testsets(path_text_data, list_skills=None):\n",
    "    list_skills = list_skills if list_skills else []\n",
    "\n",
    "    test_datasets = {'all' : pd.read_csv(path_text_data + 'Caption_all.tsv', sep='\\t')}\n",
    "    test_datasets.update({\n",
    "        'test_%s' %skill : pd.read_csv(path_text_data + 'Caption_testing_%s.tsv'%skill, sep='\\t') \n",
    "        for skill in list_skills if os.path.isfile(path_text_data + 'Caption_testing_%s.tsv'%skill)})\n",
    "    test_datasets['test'] = test_datasets['all'][test_datasets['all'].split == 'test']\n",
    "    test_datasets['val'] = test_datasets['all'][test_datasets['all'].split == 'val']\n",
    "    del test_datasets['all']\n",
    "\n",
    "    for split, dataset in test_datasets.items():\n",
    "        dataset['caption'] = dataset['caption'].apply(pre_caption)\n",
    "        dataset['image_id'] = dataset['image_ID'].astype(str)\n",
    "\n",
    "    for split in test_datasets:\n",
    "        test_datasets[split] = test_datasets[split][['image_id','caption']]\n",
    "\n",
    "    return test_datasets\n",
    "\n",
    "def to_grouped_dict(dataset):\n",
    "    dict_dataset = dataset.to_dict(orient='records')\n",
    "    grouped_dataset = defaultdict(list)\n",
    "    for element in dict_dataset:\n",
    "        grouped_dataset[element['image_id']].append(element['caption'])\n",
    "\n",
    "    return grouped_dataset\n",
    "\n",
    "def build_lists_for_evaluation(results, test_dataset):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for image_id in test_dataset:\n",
    "        predictions.append(results[image_id])\n",
    "        references.append(test_dataset[image_id])\n",
    "    return predictions, references"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-dealing",
   "metadata": {},
   "source": [
    "### Load References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bacterial-auction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1007129816</td>\n",
       "      <td>the man with pierced ears is wearing glasses a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1007129816</td>\n",
       "      <td>a man with glasses is wearing a beer can croch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>1007129816</td>\n",
       "      <td>a man with gauges and glasses is wearing a bli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>1007129816</td>\n",
       "      <td>a man in an orange hat starring at something</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1007129816</td>\n",
       "      <td>a man wears an orange hat and glasses</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id                                            caption\n",
       "125  1007129816  the man with pierced ears is wearing glasses a...\n",
       "126  1007129816  a man with glasses is wearing a beer can croch...\n",
       "127  1007129816  a man with gauges and glasses is wearing a bli...\n",
       "128  1007129816       a man in an orange hat starring at something\n",
       "129  1007129816              a man wears an orange hat and glasses"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_text_data = './data/'\n",
    "list_skills = ['color','counting','gender']\n",
    "test_datasets = load_testsets(path_text_data, list_skills=list_skills)\n",
    "test_datasets['test'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "brave-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in test_datasets:\n",
    "    test_datasets[split] = to_grouped_dict(test_datasets[split])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-organic",
   "metadata": {},
   "source": [
    "### Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "distributed-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import trange, tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fd88f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.meteor_score import meteor_score\n",
    "import numpy as np\n",
    "cap_metrics = evaluate.combine(['bleu', 'rouge'])\n",
    "\n",
    "def compute_metrics(predictions, references):\n",
    "    metrics = cap_metrics.compute(predictions=predictions, references=references)\n",
    "    for i in range(4):\n",
    "        metrics[f'bleu{i+1}'] = metrics['precisions'][i]\n",
    "    metrics['meteor'] = np.mean([meteor_score(hypothesis=p, references=rs) for p, rs in zip(predictions, references)])\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "secret-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'BLIP/output/'\n",
    "exp_names = [\n",
    "    str(dir_.stem) for dir_ in Path(base_dir).glob('*') if str(dir_.stem) not in ['saved_exps', '.gitignore']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "stone-snapshot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caption_base_flickr',\n",
       " 'caption_flickr_aae_color',\n",
       " 'caption_flickr_aae_counting',\n",
       " 'caption_flickr_aae_gender',\n",
       " 'caption_flickr_aae_color+counting+gender']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_names = [exp for exp in exp_names if 'aae' in exp or 'base' in exp]\n",
    "exp_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "industrial-lloyd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdb15e21bd694b19b22a73eeb54014f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4adb3860653449ef9694473a292a6b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39f3e864a2041bca89636ef03afa12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3c8e3bda1c42deb0d6c069dd97a894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    }
   ],
   "source": [
    "metrics = {}\n",
    "for exp_name in exp_names:\n",
    "    result_dir = Path(f'{base_dir}/{exp_name}/result')\n",
    "    pbar = tqdm(list(result_dir.glob('test_epoch*.json')))\n",
    "    metrics[exp_name] = defaultdict(list)\n",
    "    for result_path in pbar:\n",
    "        if 'rank' in str(result_path):\n",
    "            continue\n",
    "        epoch = int(result_path.stem.replace('test_epoch', ''))\n",
    "        with result_path.open() as fp:\n",
    "            raw_results = json.load(fp)\n",
    "\n",
    "        results = group_results(raw_results)\n",
    "        for split in test_datasets:\n",
    "            if split == 'val':\n",
    "                continue\n",
    "            predictions, references = build_lists_for_evaluation(results, test_datasets[split])\n",
    "\n",
    "            pbar.set_description(f\"{exp_name}: {split}\")\n",
    "            computed_metric = compute_metrics(predictions=predictions,\n",
    "                                              references=references)\n",
    "\n",
    "            metrics[exp_name][split].append(computed_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "suburban-flood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0657b703e4f84d20b776bb19dde2aeb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a969ec201d1484495d0c88094215bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af166a9b33884edab5b4962a960079b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0ee79a323b4de2a2ad06ddef9a17fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab25b2f2df74ba3aef3eb681e59d467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    }
   ],
   "source": [
    "for exp_name in exp_names:\n",
    "    result_dir = Path(f'{base_dir}/{exp_name}/result')\n",
    "    pbar = tqdm(list(result_dir.glob('val_epoch*.json')))\n",
    "    for result_path in pbar:\n",
    "        if 'rank' in str(result_path):\n",
    "            continue\n",
    "        epoch = int(result_path.stem.replace('val_epoch', ''))\n",
    "        with result_path.open() as fp:\n",
    "            raw_results = json.load(fp)\n",
    "\n",
    "        results = group_results(raw_results)\n",
    "\n",
    "        split = 'val'\n",
    "        predictions, references = build_lists_for_evaluation(results, test_datasets[split])\n",
    "\n",
    "        pbar.set_description(f\"{exp_name}: {split}\")\n",
    "        computed_metric = compute_metrics(predictions=predictions,\n",
    "                                          references=references)\n",
    "\n",
    "        metrics[exp_name][split].append(computed_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "copyrighted-documentary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs registered for experiment caption_base_flickr:\n",
      "test_color  : 10\n",
      "test_countin: 10\n",
      "test_gender : 10\n",
      "test        : 10\n",
      "val         : 10\n",
      "Epochs registered for experiment caption_flickr_aae_color:\n",
      "test_color  : 12\n",
      "test_countin: 12\n",
      "test_gender : 12\n",
      "test        : 12\n",
      "val         : 12\n",
      "Epochs registered for experiment caption_flickr_aae_counting:\n",
      "test_color  : 9\n",
      "test_countin: 9\n",
      "test_gender : 9\n",
      "test        : 9\n",
      "val         : 9\n",
      "Epochs registered for experiment caption_flickr_aae_gender:\n",
      "test_color  : 12\n",
      "test_countin: 12\n",
      "test_gender : 12\n",
      "test        : 12\n",
      "val         : 12\n",
      "Epochs registered for experiment caption_flickr_aae_color+counting+gender:\n",
      "test_color  : 9\n",
      "test_countin: 9\n",
      "test_gender : 9\n",
      "test        : 9\n",
      "val         : 9\n"
     ]
    }
   ],
   "source": [
    "for exp_name in metrics:\n",
    "    print(f'Epochs registered for experiment {exp_name}:')\n",
    "    for split in metrics[exp_name]:\n",
    "        print(f'{split:12.12s}:', len(metrics[exp_name][split]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-introduction",
   "metadata": {},
   "source": [
    "### Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "composed-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold(text):\n",
    "    BOLD = '\\033[1m'\n",
    "    END = '\\033[0m'\n",
    "    return BOLD + text + END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "severe-budget",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-782b1b20fd30>, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-782b1b20fd30>\"\u001b[0;36m, line \u001b[0;32m23\u001b[0m\n\u001b[0;31m    'caption_flickr_aae_color': 'ACol',\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "display_name = {\n",
    "    'caption_base_flickr': 'Base',\n",
    "    \n",
    "    'caption_flickr_augmented_c': 'Col',\n",
    "    'caption_flickr_augmented_counting': 'Cnt',\n",
    "    'caption_augmented_flickr': 'Gen',\n",
    "\n",
    "    'caption_flickr_augmented_color+counting': 'Col+Cnt',\n",
    "    'caption_flickr_augmented_c+g': 'Col+Gen',\n",
    "    'caption_flickr_augmented_counting+gender': 'Cnt+Gen',\n",
    "    'caption_flickr_augmented_color+counting+gender': 'C+C+G',\n",
    "\n",
    "    \n",
    "    'caption_flickr_inpaiting_color': 'ICol',\n",
    "    'caption_flickr_inpaiting_counting': 'ICnt',\n",
    "    'caption_flickr_inpaiting_gender': 'IGen',\n",
    "\n",
    "    'caption_flickr_inpaiting_color+counting': 'ICol+Cnt',\n",
    "    'caption_flickr_inpaiting_color+gender': 'ICol+Gen',\n",
    "    'caption_flickr_inpaiting_counting+gender': 'ICnt+Gen',\n",
    "    'caption_flickr_inpaiting_color+counting+gender': 'IC+C+G'\n",
    "    \n",
    "    'caption_flickr_aae_color': 'ACol',\n",
    "    'caption_flickr_aae_counting': 'ACnt',\n",
    "    'caption_flickr_aae_gender': 'AGen',\n",
    "    'caption_flickr_aae_color+counting+gender': 'AC+C+G'\n",
    "\n",
    "}\n",
    "relevant_metrics = ['bleu', 'bleu1', 'bleu2', 'bleu3', 'bleu4', 'rouge1', 'rouge2', 'rougeL', 'rougeLsum', 'meteor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed847e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "best_epochs = {}\n",
    "for exp_name in exp_names:\n",
    "    ckpt_path = f'{base_dir}/{exp_name}/checkpoint_best.pth'\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        continue\n",
    "    ckpt = torch.load(ckpt_path)\n",
    "    best_epochs[exp_name] = ckpt['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc6f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e722ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for split in ['val', 'test', 'test_gender', 'test_color']:\n",
    "# for exp_name in ['caption_base_flickr','caption_augmented_flickr','caption_flickr_augmented_c','caption_flickr_augmented_c+g']:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-wright",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_test = ['val', 'test', 'test_color', 'test_counting', 'test_gender']\n",
    "sort_name = list(display_name.keys())\n",
    "\n",
    "selected_exp_names = [name for name in exp_names if '+' not in name] # non composite train sets\n",
    "# selected_exp_names = exp_names                                       # all train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in sorted(test_datasets.keys(), key=sort_test.index):\n",
    "    heading = f'{split:11.11s}'\n",
    "\n",
    "    for exp_name in sorted(selected_exp_names, key=sort_name.index):\n",
    "        exp_display_name = display_name.get(exp_name, exp_name)\n",
    "        heading += f'{exp_display_name:>9.8s}'\n",
    "    print(bold(heading))\n",
    "\n",
    "    for m in relevant_metrics:\n",
    "        row = f'{m:10.10s}:'\n",
    "        highest_metric = float('-inf')\n",
    "\n",
    "        for exp_name in sorted(selected_exp_names, key=sort_name.index):\n",
    "            if exp_name not in best_epochs:\n",
    "                continue\n",
    "            best_epoch = best_epochs[exp_name]\n",
    "            instance_metrics = metrics[exp_name][split][best_epoch]   \n",
    "            highest_metric = max(highest_metric, metrics[exp_name][split][best_epoch][m])\n",
    "\n",
    "        for exp_name in sorted(selected_exp_names, key=sort_name.index):\n",
    "            if exp_name not in best_epochs:\n",
    "                continue\n",
    "            best_epoch = best_epochs[exp_name]\n",
    "            instance_metrics = metrics[exp_name][split][best_epoch]\n",
    "            metric_str = f'{instance_metrics[m]:9.4f}'\n",
    "            if instance_metrics[m] == highest_metric:\n",
    "                metric_str = bold(metric_str)\n",
    "            row += metric_str\n",
    "            \n",
    "        print(row)\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-process",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_name in exp_names:\n",
    "    val_bleu4 = [m['bleu4'] for m in metrics[exp_name][split]]\n",
    "    plt.plot(val_bleu4, label=display_name[exp_name])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-childhood",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-referral",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-aggregate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-inspiration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-glance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-writing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-fraction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-antenna",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-combination",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
