{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../BLIP/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-brother",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_id(composit_image_id):\n",
    "    composit_image_id = composit_image_id.split('_')\n",
    "    if len(composit_image_id) > 2:\n",
    "        image_id = composit_image_id[-2]\n",
    "    else:\n",
    "        image_id = composit_image_id[-1]\n",
    "    return image_id\n",
    "\n",
    "def get_split(composit_image_id):\n",
    "    composit_image_id = composit_image_id.split('_')\n",
    "    if len(composit_image_id) > 2:\n",
    "        composit_split = composit_image_id[:-2]\n",
    "    else:\n",
    "        composit_split = composit_image_id[:-1]\n",
    "    \n",
    "    split = '_'.join(composit_split)\n",
    "    return split\n",
    "\n",
    "def group_results(raw_results):\n",
    "    results = {}\n",
    "    for element in raw_results:\n",
    "        image_id = get_image_id(element['image_id'])\n",
    "        split = get_split(element['image_id'])\n",
    "        results[image_id] = element['caption']\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def pre_caption(caption, max_words=0):\n",
    "    caption = re.sub(\n",
    "        r\"([.!\\\"()*#:;~])\",\n",
    "        ' ',\n",
    "        caption.lower(),\n",
    "    )\n",
    "    caption = re.sub(\n",
    "        r\"\\s{2,}\",\n",
    "        ' ',\n",
    "        caption,\n",
    "    )\n",
    "    caption = caption.rstrip('\\n')\n",
    "    caption = caption.strip(' ')\n",
    "\n",
    "    #truncate caption\n",
    "    caption_words = caption.split(' ')\n",
    "    if max_words and len(caption_words)>max_words:\n",
    "        caption = ' '.join(caption_words[:max_words])\n",
    "\n",
    "    return caption\n",
    "\n",
    "def load_testsets(path_text_data, list_skills=None):\n",
    "    list_skills = list_skills if list_skills else []\n",
    "\n",
    "    test_datasets = {'all' : pd.read_csv(path_text_data + 'Caption_all.tsv', sep='\\t')}\n",
    "    test_datasets.update({\n",
    "        'test_%s' %skill : pd.read_csv(path_text_data + 'Caption_testing_%s.tsv'%skill, sep='\\t') \n",
    "        for skill in list_skills if os.path.isfile(path_text_data + 'Caption_testing_%s.tsv'%skill)})\n",
    "    test_datasets['test'] = test_datasets['all'][test_datasets['all'].split == 'test']\n",
    "    test_datasets['val'] = test_datasets['all'][test_datasets['all'].split == 'val']\n",
    "    del test_datasets['all']\n",
    "\n",
    "    for split, dataset in test_datasets.items():\n",
    "        dataset['caption'] = dataset['caption'].apply(pre_caption)\n",
    "        dataset['image_id'] = dataset['image_ID'].astype(str)\n",
    "\n",
    "    for split in test_datasets:\n",
    "        test_datasets[split] = test_datasets[split][['image_id','caption']]\n",
    "\n",
    "    return test_datasets\n",
    "\n",
    "def to_grouped_dict(dataset):\n",
    "    dict_dataset = dataset.to_dict(orient='records')\n",
    "    grouped_dataset = defaultdict(list)\n",
    "    for element in dict_dataset:\n",
    "        grouped_dataset[element['image_id']].append(element['caption'])\n",
    "\n",
    "    return grouped_dataset\n",
    "\n",
    "def build_lists_for_evaluation(results, test_dataset):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for image_id in test_dataset:\n",
    "        predictions.append(results[image_id])\n",
    "        references.append(test_dataset[image_id])\n",
    "    return predictions, references"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-dealing",
   "metadata": {},
   "source": [
    "### Load References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_text_data = './'\n",
    "list_skills = ['gender','color']\n",
    "test_datasets = load_testsets(path_text_data, list_skills=list_skills)\n",
    "test_datasets['test'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in test_datasets:\n",
    "    test_datasets[split] = to_grouped_dict(test_datasets[split])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-organic",
   "metadata": {},
   "source": [
    "### Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import trange, tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd88f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.meteor_score import meteor_score\n",
    "import numpy as np\n",
    "cap_metrics = evaluate.combine(['bleu', 'rouge'])\n",
    "\n",
    "def compute_metrics(predictions, references):\n",
    "    metrics = cap_metrics.compute(predictions=predictions, references=references)\n",
    "    for i in range(4):\n",
    "        metrics[f'bleu{i+1}'] = metrics['precisions'][i]\n",
    "    metrics['meteor'] = np.mean([meteor_score(hypothesis=p, references=rs) for p, rs in zip(predictions, references)])\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../BLIP/output/'\n",
    "exp_names = [str(dir_.stem) for dir_ in Path(base_dir).glob('*') if str(dir_.stem) != 'saved_exps']\n",
    "trained_epochs = 5\n",
    "\n",
    "metrics = {}\n",
    "for exp_name in exp_names:\n",
    "    result_dir = Path(f'{base_dir}/{exp_name}/result')\n",
    "    pbar = tqdm(list(result_dir.glob('test_epoch*.json')))\n",
    "    metrics[exp_name] = defaultdict(list)\n",
    "    for result_path in pbar:\n",
    "        if 'rank' in str(result_path):\n",
    "            continue\n",
    "        epoch = int(result_path.stem.replace('test_epoch', ''))\n",
    "        with result_path.open() as fp:\n",
    "            raw_results = json.load(fp)\n",
    "\n",
    "        results = group_results(raw_results)\n",
    "        for split in test_datasets:\n",
    "            if split == 'val':\n",
    "                continue\n",
    "            predictions, references = build_lists_for_evaluation(results, test_datasets[split])\n",
    "\n",
    "            pbar.set_description(f\"{exp_name}: {split}\")\n",
    "            computed_metric = compute_metrics(predictions=predictions,\n",
    "                                              references=references)\n",
    "\n",
    "            metrics[exp_name][split].append(computed_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_name in exp_names:\n",
    "    result_dir = Path(f'{base_dir}/{exp_name}/result')\n",
    "    pbar = tqdm(list(result_dir.glob('val_epoch*.json')))\n",
    "    for result_path in pbar:\n",
    "        if 'rank' in str(result_path):\n",
    "            continue\n",
    "        epoch = int(result_path.stem.replace('val_epoch', ''))\n",
    "        with result_path.open() as fp:\n",
    "            raw_results = json.load(fp)\n",
    "\n",
    "        results = group_results(raw_results)\n",
    "\n",
    "        split = 'val'\n",
    "        predictions, references = build_lists_for_evaluation(results, test_datasets[split])\n",
    "\n",
    "        pbar.set_description(f\"{exp_name}: {split}\")\n",
    "        computed_metric = compute_metrics(predictions=predictions,\n",
    "                                          references=references)\n",
    "\n",
    "        metrics[exp_name][split].append(computed_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-documentary",
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_name in metrics:\n",
    "    print(f'Epochs registered for experiment {exp_name}:')\n",
    "    for split in metrics[exp_name]:\n",
    "        print(f'{split:12.12s}:', len(metrics[exp_name][split]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-introduction",
   "metadata": {},
   "source": [
    "### Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold(text):\n",
    "    BOLD = '\\033[1m'\n",
    "    END = '\\033[0m'\n",
    "    return BOLD + text + END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_name = {\n",
    "    'caption_base_flickr': 'Base',\n",
    "    'caption_augmented_flickr': 'Gender',\n",
    "    'caption_flickr_augmented_c': 'Color',\n",
    "    'caption_flickr_augmented_c+g': 'Col+Gen',\n",
    "}\n",
    "relevant_metrics = ['bleu', 'bleu1', 'bleu2', 'bleu3', 'bleu4', 'rouge1', 'rouge2', 'rougeL', 'rougeLsum', 'meteor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed847e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "best_epochs = {}\n",
    "for exp_name in exp_names:\n",
    "    ckpt_path = f'{base_dir}/{exp_name}/checkpoint_best.pth'\n",
    "    ckpt = torch.load(ckpt_path)\n",
    "    best_epochs[exp_name] = ckpt['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc6f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e722ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for split in ['val', 'test', 'test_gender', 'test_color']:\n",
    "# for exp_name in ['caption_base_flickr','caption_augmented_flickr','caption_flickr_augmented_c','caption_flickr_augmented_c+g']:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in test_datasets.keys():\n",
    "    heading = f'{split:11.11s}'\n",
    "    for exp_name in exp_names:\n",
    "        exp_display_name = display_name.get(exp_name,exp_name)\n",
    "        heading += f'{exp_display_name:>10.8s}'\n",
    "    print(bold(heading))\n",
    "    for m in relevant_metrics:\n",
    "        row = f'{m:10.10s}:'\n",
    "        highest_metric = float('-inf')\n",
    "        for exp_name in exp_names:\n",
    "            best_epoch = best_epochs[exp_name]\n",
    "            instance_metrics = metrics[exp_name][split][best_epoch]   \n",
    "            highest_metric = max(highest_metric, metrics[exp_name][split][best_epoch][m])\n",
    "        for exp_name in exp_names:\n",
    "            best_epoch = best_epochs[exp_name]\n",
    "            instance_metrics = metrics[exp_name][split][best_epoch]\n",
    "            metric_str = f'{instance_metrics[m]:10.4f}'\n",
    "            if instance_metrics[m] == highest_metric:\n",
    "                metric_str = bold(metric_str)\n",
    "            row += metric_str\n",
    "            \n",
    "        print(row)\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-process",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_name in exp_names:\n",
    "    val_bleu4 = [m['bleu4'] for m in metrics[exp_name][split]]\n",
    "    plt.plot(val_bleu4, label=display_name[exp_name])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-childhood",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-referral",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-aggregate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
